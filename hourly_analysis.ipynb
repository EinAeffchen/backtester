{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Close     High      Low     Open  Volume  \\\n",
      "Datetime                                                                \n",
      "2023-02-22 19:00:00+00:00  3989.25  4025.00  3988.50  4016.75  367633   \n",
      "2023-02-22 20:00:00+00:00  4000.00  4001.25  3983.75  3989.25  314003   \n",
      "2023-02-23 08:00:00+00:00  4018.25  4023.25  4013.50  4014.50   28416   \n",
      "2023-02-23 09:00:00+00:00  4015.25  4022.75  4009.25  4018.50   23686   \n",
      "2023-02-23 10:00:00+00:00  4015.25  4016.50  4011.75  4015.50   13246   \n",
      "\n",
      "                                 20_MA    20_STD       200_MA        50_MA  \\\n",
      "Datetime                                                                     \n",
      "2023-02-22 19:00:00+00:00  4008.482143  6.988634  4008.482143  4008.482143   \n",
      "2023-02-22 20:00:00+00:00  4007.916667  7.081582  4007.916667  4007.916667   \n",
      "2023-02-23 08:00:00+00:00  4008.562500  7.312945  4008.562500  4008.562500   \n",
      "2023-02-23 09:00:00+00:00  4008.955882  7.264122  4008.955882  4008.955882   \n",
      "2023-02-23 10:00:00+00:00  4009.305556  7.201693  4009.305556  4009.305556   \n",
      "\n",
      "                           Divergence  ...           S1           R1  \\\n",
      "Datetime                               ...                             \n",
      "2023-02-22 19:00:00+00:00        True  ...  4000.166667  4014.166667   \n",
      "2023-02-22 20:00:00+00:00       False  ...  4000.166667  4014.166667   \n",
      "2023-02-23 08:00:00+00:00       False  ...  3980.833333  4022.083333   \n",
      "2023-02-23 09:00:00+00:00        True  ...  3980.833333  4022.083333   \n",
      "2023-02-23 10:00:00+00:00       False  ...  3980.833333  4022.083333   \n",
      "\n",
      "                                    S2           R2  Dist_Pivot    Dist_R1  \\\n",
      "Datetime                                                                     \n",
      "2023-02-22 19:00:00+00:00  3994.083333  4022.083333  -18.833333 -24.916667   \n",
      "2023-02-22 20:00:00+00:00  3994.083333  4022.083333   -8.083333 -14.166667   \n",
      "2023-02-23 08:00:00+00:00  3961.666667  4044.166667   15.333333  -3.833333   \n",
      "2023-02-23 09:00:00+00:00  3961.666667  4044.166667   12.333333  -6.833333   \n",
      "2023-02-23 10:00:00+00:00  3961.666667  4044.166667   12.333333  -6.833333   \n",
      "\n",
      "                              ATR_14  Norm_Dist_Pivot  Norm_Dist_R1  \\\n",
      "Datetime                                                              \n",
      "2023-02-22 19:00:00+00:00  14.660714        -1.284612     -1.699553   \n",
      "2023-02-22 20:00:00+00:00  14.982143        -0.539531     -0.945570   \n",
      "2023-02-23 08:00:00+00:00  14.821429         1.034538     -0.258635   \n",
      "2023-02-23 09:00:00+00:00  14.535714         0.848485     -0.470106   \n",
      "2023-02-23 10:00:00+00:00  14.089286         0.875370     -0.485002   \n",
      "\n",
      "                           Market_Segment  \n",
      "Datetime                                   \n",
      "2023-02-22 19:00:00+00:00               3  \n",
      "2023-02-22 20:00:00+00:00               0  \n",
      "2023-02-23 08:00:00+00:00               0  \n",
      "2023-02-23 09:00:00+00:00               1  \n",
      "2023-02-23 10:00:00+00:00               1  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Reload the dataset\n",
    "file_path = \"data/gspc_hourly_with_indicators.csv\"\n",
    "df = pd.read_csv(file_path, index_col=0, parse_dates=[0])\n",
    "\n",
    "df['Date'] = df.index.date  # Extract date to group by day\n",
    "\n",
    "# Compute previous day's High, Low, Close\n",
    "daily_pivot_data = df.groupby('Date').agg({'High': 'max', 'Low': 'min', 'Close': 'last'}).shift(1)\n",
    "\n",
    "# Merge back to hourly data, ensuring the pivot points remain constant throughout the day\n",
    "df = df.merge(daily_pivot_data, left_on='Date', right_index=True, suffixes=('', '_prev_day'))\n",
    "\n",
    "# Calculate Corrected Daily Pivot Points\n",
    "df['Pivot'] = (df['High_prev_day'] + df['Low_prev_day'] + df['Close_prev_day']) / 3\n",
    "df['S1'] = (2 * df['Pivot']) - df['High_prev_day']\n",
    "df['R1'] = (2 * df['Pivot']) - df['Low_prev_day']\n",
    "df['S2'] = df['Pivot'] - (df['High_prev_day'] - df['Low_prev_day'])\n",
    "df['R2'] = df['Pivot'] + (df['High_prev_day'] - df['Low_prev_day'])\n",
    "\n",
    "# Drop columns used for calculations to keep dataset clean\n",
    "df.drop(columns=['High_prev_day', 'Low_prev_day', 'Close_prev_day'], inplace=True)\n",
    "\n",
    "# Compute distances from pivot points\n",
    "df['Dist_Pivot'] = df['Close'] - df['Pivot']\n",
    "df['Dist_R1'] = df['Close'] - df['R1']\n",
    "\n",
    "# Compute ATR (for normalization)\n",
    "df['ATR_14'] = (df['High'] - df['Low']).rolling(window=14).mean()\n",
    "\n",
    "# Normalize distances using ATR\n",
    "df['Norm_Dist_Pivot'] = df['Dist_Pivot'] / df['ATR_14']\n",
    "df['Norm_Dist_R1'] = df['Dist_R1'] / df['ATR_14']\n",
    "\n",
    "# Compute 'Change', 'Slope', 'Acceleration'\n",
    "df['Change'] = df['Close'].pct_change()\n",
    "df['Slope'] = df['Close'].diff()\n",
    "df['Acceleration'] = df['Slope'].diff()\n",
    "\n",
    "# Drop NaN values resulting from shifting and rolling calculations\n",
    "selected_features = ['Change', 'Slope', 'Acceleration', 'Norm_Dist_Pivot', 'Norm_Dist_R1']\n",
    "df.dropna(subset=selected_features, inplace=True)\n",
    "\n",
    "# Standardize the selected features for clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[selected_features])\n",
    "\n",
    "# Apply KMeans clustering (choosing 4 clusters based on Elbow Method)\n",
    "optimal_clusters = 4\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
    "df['Market_Segment'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Display the updated dataset with clusters\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   gamma   | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.4734   \u001b[39m | \u001b[39m1.873    \u001b[39m | \u001b[39m0.2857   \u001b[39m | \u001b[39m8.124    \u001b[39m | \u001b[39m219.7    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.4448   \u001b[39m | \u001b[39m0.7801   \u001b[39m | \u001b[39m0.05524  \u001b[39m | \u001b[39m3.407    \u001b[39m | \u001b[39m273.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.4009   \u001b[39m | \u001b[39m3.006    \u001b[39m | \u001b[39m0.2153   \u001b[39m | \u001b[39m3.144    \u001b[39m | \u001b[39m294.0    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.405    \u001b[39m | \u001b[39m4.162    \u001b[39m | \u001b[39m0.07158  \u001b[39m | \u001b[39m4.273    \u001b[39m | \u001b[39m136.7    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.4617   \u001b[39m | \u001b[39m1.521    \u001b[39m | \u001b[39m0.1622   \u001b[39m | \u001b[39m6.024    \u001b[39m | \u001b[39m158.2    \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.492    \u001b[39m | \u001b[35m0.8442   \u001b[39m | \u001b[35m0.2483   \u001b[39m | \u001b[35m8.385    \u001b[39m | \u001b[35m219.3    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.4869   \u001b[39m | \u001b[39m0.1473   \u001b[39m | \u001b[39m0.1911   \u001b[39m | \u001b[39m9.927    \u001b[39m | \u001b[39m217.4    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.4762   \u001b[39m | \u001b[39m0.503    \u001b[39m | \u001b[39m0.07347  \u001b[39m | \u001b[39m5.969    \u001b[39m | \u001b[39m216.3    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.4896   \u001b[39m | \u001b[39m0.127    \u001b[39m | \u001b[39m0.01535  \u001b[39m | \u001b[39m8.335    \u001b[39m | \u001b[39m225.0    \u001b[39m |\n",
      "| \u001b[35m10       \u001b[39m | \u001b[35m0.4944   \u001b[39m | \u001b[35m0.06661  \u001b[39m | \u001b[35m0.234    \u001b[39m | \u001b[35m3.783    \u001b[39m | \u001b[35m222.5    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.4252   \u001b[39m | \u001b[39m1.394    \u001b[39m | \u001b[39m0.1442   \u001b[39m | \u001b[39m3.629    \u001b[39m | \u001b[39m228.7    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.491    \u001b[39m | \u001b[39m0.3456   \u001b[39m | \u001b[39m0.06056  \u001b[39m | \u001b[39m9.859    \u001b[39m | \u001b[39m209.2    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.4924   \u001b[39m | \u001b[39m0.04777  \u001b[39m | \u001b[39m0.1879   \u001b[39m | \u001b[39m5.407    \u001b[39m | \u001b[39m207.4    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.4712   \u001b[39m | \u001b[39m1.896    \u001b[39m | \u001b[39m0.06064  \u001b[39m | \u001b[39m9.168    \u001b[39m | \u001b[39m204.2    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.422    \u001b[39m | \u001b[39m4.278    \u001b[39m | \u001b[39m0.2513   \u001b[39m | \u001b[39m6.932    \u001b[39m | \u001b[39m210.0    \u001b[39m |\n",
      "=========================================================================\n",
      "🔎 Best Found Hyperparameters: {'gamma': np.float64(0.06661174974660289), 'learning_rate': np.float64(0.23398377960349634), 'max_depth': 3, 'n_estimators': 222}\n",
      "     Metric  Mean Value\n",
      "0  Accuracy    0.513221\n",
      "1  F1 Score    0.494350\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "\n",
    "# Select features\n",
    "selected_features = ['Change', 'Slope', 'Acceleration', 'Norm_Dist_Pivot', 'Norm_Dist_R1', 'Market_Segment']\n",
    "X = df[selected_features]\n",
    "y = df['Target']\n",
    "\n",
    "# Time Series Split (Ensures No Data Leakage)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# 🚀 Step 1: Bayesian Optimization Function\n",
    "def xgb_evaluate(n_estimators, learning_rate, max_depth, gamma):\n",
    "    model = XGBClassifier(n_estimators=int(n_estimators), \n",
    "                          learning_rate=learning_rate, \n",
    "                          max_depth=int(max_depth),\n",
    "                          gamma=gamma,\n",
    "                          random_state=42, \n",
    "                          eval_metric='mlogloss')\n",
    "    \n",
    "    accuracies, f1_scores = [], []\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # 🚀 Apply SMOTE to balance the classes in training data\n",
    "        smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Compute class weights\n",
    "        sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_resampled)\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_resampled, y_train_resampled, sample_weight=sample_weights)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate model\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    \n",
    "    # Return the average F1-score (Bayesian Optimization maximizes this)\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# 🚀 Step 2: Run Bayesian Optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_evaluate,\n",
    "    pbounds={\n",
    "        \"n_estimators\": (100, 300),  # Number of trees\n",
    "        \"learning_rate\": (0.01, 0.3),  # Learning rate\n",
    "        \"max_depth\": (3, 10),  # Tree depth\n",
    "        \"gamma\": (0, 5)  # Regularization\n",
    "    },\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Run Optimization (10 iterations)\n",
    "optimizer.maximize(n_iter=10)\n",
    "\n",
    "# 🚀 Step 3: Get the Best Parameters\n",
    "best_params = optimizer.max['params']\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])  # Convert to integer\n",
    "best_params['max_depth'] = int(best_params['max_depth'])  # Convert to integer\n",
    "\n",
    "print(\"🔎 Best Found Hyperparameters:\", best_params)\n",
    "\n",
    "# 🚀 Step 4: Train Model with Optimized Parameters\n",
    "model = XGBClassifier(**best_params, random_state=42, eval_metric='mlogloss')\n",
    "\n",
    "accuracies, f1_scores = [], []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # 🚀 Apply SMOTE\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Compute class weights\n",
    "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_resampled)\n",
    "    \n",
    "    # Train with best parameters\n",
    "    model.fit(X_train_resampled, y_train_resampled, sample_weight=sample_weights)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# 🚀 Step 5: Display Final Model Performance\n",
    "model_performance = pd.DataFrame({'Metric': ['Accuracy', 'F1 Score'], \n",
    "                                  'Mean Value': [np.mean(accuracies), np.mean(f1_scores)]})\n",
    "\n",
    "print(model_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "1    4594\n",
       "0    1088\n",
       "2    1085\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
